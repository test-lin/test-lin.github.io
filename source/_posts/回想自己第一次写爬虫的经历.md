---
title: 回想自己第一次写爬虫的经历
date: 2019-04-13 17:56:34
updated: 2019-04-13 17:56:34
categories:
- 回忆录
comments: true
---

## 起因

第一次爬虫是因为觉得上班不忙，又不想每天玩游戏让自己后悔。所以以锻炼为目的写爬虫。因为自己尿性是没有毅力也很容易放弃的人。所以我选择的内容是抓小网站，让自己的学习不会太过于困难。那时并没有很多开发经验，也没有做所谓的启动准备。都是想到什么就去做什么，也不知道自己这个项目要做到什么程度。

## 实现经过

我的想法很简单，取得网页，匹配网页想要的内容并保存起来。因为技能问题，我没有使用 python 来抓取。我用的是 php。

最开始，我使用 file_get_content() 函数来取得网页，然后用正则匹配来匹配内容，但是那太慢了，然后我就去搜了一下，使用了 curl 函数来代替了 file_get_content。

我抓取的网站有三种内容：文章、图组和视频

文章只需要抓取网页就可以取得内容了。但是图组不但要取得网页，还要匹配到的图片下载到本地中。当时完全就是写一个脚本文件，然后就执行。所以我分成两步脚本进行处理。但是总是被大大小小的问题卡往。后面也只做了文章和图组的抓取，也没有整理维护了。

## 碰到的问题

### 字符编码

因为有些文章并不是使用 utf-8 编码，所以我们需要通过 mb_detect_encoding 函数检查网页的字符编码并用 iconv 函数转成 utf-8 的格式。

### 内存占用越过了 php.ini 的限制，导致报异常程序中断

最开始我的想法是把所有得到的内容先存放在变量中，到结束后再进行一些性保存，所以导致变量存了太多数据

### 资源重复

编码时没有考虑到脚本中断的情况，每次都是从头开始。还有一些图组资源是本来就是重复的，

### 脚本运行卡死

下载时有网络波动、电脑休眠都会导致卡死。这种情况要么就想办法处理网络波动和电脑的问题，要么就做一些中断命令后重新运行会接着执行的操作。

### 数据丢失

文章内容存在数据库，使用的是 text 类型字段，但是抓取的内容超过了范围导致内容遗失。可以有以下 2 种办法去解决这种问题。

1. 设置合适的字段
2. 切换一种内容存储方式，如存成文件

## 扩展思考

### 模拟登录

这里涉及到 session 原理。传统的网站一般都使用 session 原理来实现登录绑定。所以我们只需要把登录成功后的所有 cookie 带上，并允许 cookie 改变即可。 curl 可以进行设置

### 定时抓取

使用定时任务，并把上面说的可持续查询方法使用上即可。

### 反反爬虫

有爬虫自然就会有反爬虫的机制。有的使用单一的一种机制来防止，有的又是使用混合的机制来进行制约。不管什么方式都有越过方法。主要反爬虫有：301中转、混乱的字符编码、不成对 html 标签以及请求速率限制。
